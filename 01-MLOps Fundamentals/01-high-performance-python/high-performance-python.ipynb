{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydXl0YUqS384"
   },
   "source": [
    "## Multi-Process One-Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZQ7oK9MTGrw"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGOGU7vVTJAf",
    "outputId": "5e20d79c-4bc5-41ca-b1d3-65a9d7db2577"
   },
   "outputs": [],
   "source": [
    "# Check number of available cores\n",
    "print(f\"Number of cpu cores:: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-pqLdzOTWz3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep, time\n",
    "\n",
    "# Example of a processing function\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj8DQqxb7NqN",
    "outputId": "e52c1d7b-c491-4227-9c9f-9d9059856a09"
   },
   "outputs": [],
   "source": [
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['julia', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size].to_numpy() for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Investigate process ID\n",
    "  print(f\"Process ID: {proc.pid}\")\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "for proc in procs:\n",
    "    proc.join() # Wait for the process to finish\n",
    "\n",
    "# Report total elapsed time\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI1nvsCSOzYM",
    "outputId": "0374964f-9a77-4f05-f025-f3d97a4ddfc0"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# The 2nd way to do multiprocess is via Pool #\n",
    "##############################################\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define a pool of processes\n",
    "NUM_PROCESSES = 2\n",
    "pool = multiprocessing.Pool(NUM_PROCESSES)\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    procs.append(\n",
    "        # The main process does not need to wait for this function\n",
    "        pool.apply_async(process_dataframe, args=(i, chunk))\n",
    "    )\n",
    "\n",
    "for proc in procs:\n",
    "    proc.get() # Wait for the process to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNl_vM4hWJUj"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# The 3rd way to do multiprocess is via Map #\n",
    "##############################################\n",
    "inputs = [(i, chunk) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# If you have one arg only, please use `.map` instead\n",
    "outputs = pool.starmap(\n",
    "    process_dataframe,\n",
    "    inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICbCCRWkUKHW",
    "outputId": "a35359df-998b-40ff-8403-453ea3309eb7"
   },
   "outputs": [],
   "source": [
    "# you can see that sometimes, two strings are concatenated w/o any `/n`\n",
    "# this is because all the processes writes to stdout at the same time, we need\n",
    "# to find a way to prevent other processes write to it while one is doing\n",
    "from multiprocessing import Lock\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    lock.acquire()\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")\n",
    "    lock.release()\n",
    "\n",
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['julia', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# Report total elapsed time\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax3LFgHcS93u"
   },
   "source": [
    "## Multi-Threaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt-98oQ_CRgG"
   },
   "source": [
    "Python provides the same APIs to multiprocessing with `start()` and `join()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR5SBZ1poPGE",
    "outputId": "fea68c6b-174a-47f1-c482-66f016a2bc02"
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "my_threads = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    my_thread = Thread(target=process_dataframe, args=(i, chunk,))\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roVklnzdXyvA"
   },
   "source": [
    "However, the way they share data is different. Let's take a look at the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apL2XLwTefei",
    "outputId": "68b2ccca-03f3-4b66-82ef-da3902d91627"
   },
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "# Define a lock to prevent race condition,\n",
    "# which is multiple updates into the same variable\n",
    "lock = Lock()\n",
    "\n",
    "# Share data\n",
    "shared_data = 0\n",
    "\n",
    "def increment_function():\n",
    "    global shared_data\n",
    "    with lock: # This is equal to acquire() + release()\n",
    "        shared_data += 1\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zlNXu-lEpUR",
    "outputId": "7164ee3b-5bd6-42bd-dc97-a652784f854c"
   },
   "outputs": [],
   "source": [
    "# Another way to access shared_data is via Queue\n",
    "from queue import Queue\n",
    "\n",
    "# Initialize the queue\n",
    "shared_queue = Queue()\n",
    "\n",
    "def increment_function():\n",
    "    shared_queue.put(1)\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()\n",
    "\n",
    "shared_data = 0\n",
    "while not shared_queue.empty():\n",
    "    shared_data += shared_queue.get()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQYIIevMMqpS",
    "outputId": "d6a0cab0-ebb0-4ecf-882a-ada0b071b0ee"
   },
   "outputs": [],
   "source": [
    "# Using a global variable is not a piece of cake in multiprocessing\n",
    "# as in multithreading\n",
    "import multiprocessing\n",
    "\n",
    "# Define a shared value between processes\n",
    "shared_variable = multiprocessing.Value('i', 0)\n",
    "\n",
    "def worker_function():\n",
    "    global shared_variable\n",
    "    with shared_variable.get_lock():\n",
    "        shared_variable.value += 1\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=worker_function)\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtAeq1cUQJ9E",
    "outputId": "1fac16a4-7a16-4a9d-ad7d-ab72a0ae1d1d"
   },
   "outputs": [],
   "source": [
    "# Or using queue\n",
    "from multiprocessing import Queue\n",
    "from multiprocessing import Process\n",
    "\n",
    "def worker_function(shared_queue):\n",
    "    shared_queue.put(1)\n",
    "\n",
    "shared_queue = Queue()\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "    my_proc = Process(target=worker_function, args=(shared_queue,))\n",
    "    my_proc.start()\n",
    "    procs.append(my_proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# The main process retrieves data from the queue\n",
    "# and aggregate the result\n",
    "result = 0\n",
    "while not shared_queue.empty():\n",
    "    result += shared_queue.get(1)\n",
    "\n",
    "print(f\"Results from multiprocessing queue: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVcMwStgTBIo"
   },
   "source": [
    "## One-Process One-Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfb2IrxEIMW7",
    "outputId": "3c554bd6-c8a1-42f8-9ca5-7de11d7eb3f4"
   },
   "outputs": [],
   "source": [
    "# Let's say you have one process with one thread only, how to deal with it?\n",
    "# AsyncIO helps us to do it\n",
    "import asyncio\n",
    "\n",
    "async def download_my_1st_data_func():\n",
    "    print(\"Starting my 1st data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 1st data func!\")\n",
    "\n",
    "async def download_my_2nd_data_func():\n",
    "    print(\"Starting my 2nd data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 2nd data func!\")\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(\n",
    "        asyncio.gather(\n",
    "            download_my_1st_data_func(),\n",
    "            download_my_2nd_data_func(),\n",
    "        )\n",
    "    )\n",
    "    loop.close()\n",
    "\n",
    "# Run the event loop\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
