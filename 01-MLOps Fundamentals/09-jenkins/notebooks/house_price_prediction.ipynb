{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19ccf78c-8efc-4c66-b729-dc72c62441c6",
   "metadata": {},
   "source": [
    "# House Price Prediction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b629e797-4806-4b70-b4aa-c5e223c7a833",
   "metadata": {},
   "source": [
    "In this notebook, we will work on a simple project called house price prediction. This tutorial will walk you through all the main steps from `retrieving the data`, `getting some insights` (or `EDA`) to building some models (or `modeling`). Hope that it will intest you ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25a71a-d919-42ef-8048-29284aee1ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install matplotlib seaborn\n",
    "# !pip install scikit-learn\n",
    "# !pip install lightgbm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6584ca4a-aa05-4d88-aedd-790aae2da371",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Retrieving the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f56a5980-9b42-45c0-bdb9-5581f24f94cf",
   "metadata": {},
   "source": [
    "In any ML problem, the first step is gathering all the related datasets. This data can come from a file, a database, an object storage or from a streaming storage such as Kafka, etc. To be simple, you will get it from a file in this notebook. This file was downloaded via this [link](https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQpMiIMJrmiTGarbs/edit?usp=sharing&ouid=115253717745408081083&rtpof=true&sd=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822c148-3098-40ef-aec2-c2a147fc0b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's import a library called Pandas for data analysis\n",
    "import pandas as pd # We use `as` here since we don't want to type a too long word `pandas`\n",
    "\n",
    "# Declare the data path\n",
    "datapath = \"../data/HousePricePrediction.xlsx\"\n",
    "\n",
    "# Use pandas to load the data from the datapath\n",
    "df = pd.read_excel(datapath) # df is the abbreviation form of DataFrame\n",
    "\n",
    "# Let's see how a dataframe looks like, it's just another way to see the data \n",
    "# instead of open an Excel file and see it. However, it is also equipped with \n",
    "# some functions to do analysis fast\n",
    "df.tail(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5738af1a-86d3-4cdf-a6a6-0b101d3764aa",
   "metadata": {},
   "source": [
    "This data contains 2919 records and 13 columns. Now, we need to contact a domain expert to get some information about the meaning of each column. This is not a simple step since they are often busy and not available for us to ask. But assume that they have some time, and give us the following information.\n",
    "\n",
    "- **Id:** House ID\n",
    "- **MSSubClass:** Identifies the type of dwelling ( such as a house, apartment, mobile home, houseboat, etc.) involved in the sale.\n",
    "- **MSZoning:** Identifies the general zoning classification of the sale.\n",
    "- **LotArea:** Lot size in square feet.\n",
    "- **LotConfig:** Configuration of the lot\n",
    "- **BldgType:** Type of dwelling\n",
    "- **verallCond:** Rates the overall condition of the house\n",
    "- **YearBuilt:** Original construction year\n",
    "- **YearRemodAdd:** Remodel date (same as construction date if no remodeling or additions).\n",
    "- **Exterior1st:** Exterior covering on house\n",
    "- **BsmtFinSF2:** Type 2 finished square feet.\n",
    "- **TotalBsmtSF:** Total square feet of basement area\n",
    "- **SalePrice:** To be predicted\n",
    "\n",
    "Pretty clear! It's time to get some statistics, this is an important step in the `EDA` stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56fd0ed-b080-4c26-b238-09562bbbf857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaec42-05ac-4a8e-b12b-43c396e024db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, get the real power of DataFrame. To get some basic stats (mean, min, max and percentile), all we need to do is using `.describe()`\n",
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43be8724-91b9-4e31-bc43-f7d7e660863c",
   "metadata": {},
   "source": [
    "Some one may ask what percentile means, let's look at the following image (which was taken from [this](https://data36.com/statistical-variability-standard-deviation-percentiles-histogram/)). The 25th percentile is the value below which 25% of your data points are found. For example, for the `LotArea` column, 25% of our houses have LotArea less than 7478 sq ft.\n",
    "\n",
    "![What is percentile](./imgs/percentile.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4736c1-bcee-4521-b15d-28e237effe82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uhm.., something is not correct here, since we can't calculate percentiles for columns containing other types than numeric, let's find these columns\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476450cf-d526-4c8e-8dac-25868dd9d755",
   "metadata": {},
   "source": [
    "Indeed, we find some columns like `MSZoning`, and `LotConfig` having `object` type other than `int64` and `float64`. Filter all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524af69c-b292-4a0f-833a-c2a4cef0bede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter non-numeric columns\n",
    "df.select_dtypes(exclude=[\"int64\", \"float64\"]) # Or you could use df.select_dtypes(include=[\"object\"]) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67ee72-a174-43fb-8ff7-1bfa5ad11532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_numeric_df = df.select_dtypes(exclude=[\"int64\", \"float64\"])\n",
    "\n",
    "# Let's see value frequecies at record level\n",
    "print(\"Value frequencies at record level\")\n",
    "print(non_numeric_df.value_counts())\n",
    "\n",
    "print(\"*\"*60)\n",
    "\n",
    "# And at column level\n",
    "print(\"Value frequencies at column level\")\n",
    "for col in non_numeric_df.columns:\n",
    "    print(f\"Column {col}\")\n",
    "    print(non_numeric_df.groupby(col).size().sort_values(ascending=False))\n",
    "    print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d83b0-e4a0-4c19-8433-76f90acaf6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"Uhm, I hate displaying like this, convert it to graphs\", a biz person said.\n",
    "# Okey, I'll do some visualizations\n",
    "\n",
    "# First import the following two libs which are often used for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# Our figure will have 2 rows and 2 cols\n",
    "num_fig_rows, num_fig_cols = 2, 2\n",
    "# Read more about subplots here:\n",
    "# https://python-course.eu/numerical-programming/creating-subplots-in-matplotlib.php\n",
    "fig, ax = plt.subplots(num_fig_rows, num_fig_cols, figsize=(10, 10))\n",
    "\n",
    "for row in range(num_fig_rows):\n",
    "    for col in range(num_fig_cols):\n",
    "        non_numeric_col = non_numeric_df.columns[row*num_fig_cols + col]\n",
    "        val_freq = non_numeric_df.groupby(non_numeric_col).size().to_dict()\n",
    "        ax[row, col].bar(list(val_freq.keys()), list(val_freq.values()))\n",
    "        ax[row, col].tick_params(labelrotation=45)\n",
    "        ax[row, col].set_title(non_numeric_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcc957-c876-4a64-8e73-e3476fa1b012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# That's seems to be okey for me, help me to add some figures for numeric columns also\n",
    "numeric_df = df.select_dtypes(exclude=[\"object\"]).drop(columns=[\"Id\"])\n",
    "\n",
    "# Our figure now needs 4 rows and 2 cols\n",
    "num_fig_rows, num_fig_cols = 4, 2\n",
    "# Read more about subplots here:\n",
    "# https://python-course.eu/numerical-programming/creating-subplots-in-matplotlib.php\n",
    "fig, ax = plt.subplots(num_fig_rows, num_fig_cols, figsize=(10, 10))\n",
    "\n",
    "for row in range(num_fig_rows):\n",
    "    for col in range(num_fig_cols):\n",
    "        numeric_col = numeric_df.columns[row*num_fig_cols + col]\n",
    "        ax[row, col].plot(numeric_df.groupby(numeric_col).size())\n",
    "        ax[row, col].tick_params(labelrotation=45)\n",
    "        ax[row, col].set_title(numeric_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2edba7d-5484-4b0e-9732-cf1ba9f651c9",
   "metadata": {},
   "source": [
    "There some interesting facts we can conclude here:\n",
    "- `OverallCond` values as mostly from 5, which means the houses may be not too bad\n",
    "- `LotArea` is highly left-skewed\n",
    "- Something happened in 1950 results to the reconstruction of many houses in the field `YearRemodAdd`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce40914f-427d-4e63-95ce-bb96c83b63ee",
   "metadata": {},
   "source": [
    "Now we need to know the correlation of features with the target `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2723d5b-f38d-4dc9-97ae-c57e8162a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(numeric_df.corr(),\n",
    "            cmap = 'BrBG',\n",
    "            fmt = '.2f',\n",
    "            linewidths = 2,\n",
    "            annot = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dda7c3e2-ea1e-41aa-a8e7-bea9a4a39031",
   "metadata": {},
   "source": [
    "We can clearly see `SalePrice` has a fairly strong positive correlation with `TotalBsmtSF`, `YearBuilt` and `YearRemodAdd`, which make so much sense since new and large houses are usually more expensive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7c49d67-dd27-4f76-8cd6-e87b06a20e84",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "709c4b00-6998-4585-828a-b86630320cae",
   "metadata": {},
   "source": [
    "OK, let's EDA be for now, move to the modeling setion to see if we can predict house prices correctly using our current knowledge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b894925-97cb-4ce3-8550-08d548cc212b",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are 4 main steps in modeling: `preprocessing data`. `train-val-test split`, `training models` and `evaluation`. We will walk through them step by step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97319dd5-7fd0-40d5-b069-cd8ef1ee947f",
   "metadata": {},
   "source": [
    "### 3.1. Preprocess Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d81f59-8204-4bc2-8822-c97c8dc2a4ee",
   "metadata": {},
   "source": [
    "There are a lot of `HOW` to consider in this stage, let's review popular ones: \n",
    "- **Fill (impute) missing values:** Missing values are often replaced by mean or median or by the most frequent value in the column. Another much simpler way is to drop rows containing missing values, but it can result in a dramatic decrease in the number of records.\n",
    "- **Encode categorical variables:** Categorical (non-numerical) variables are hardly fed to models directly. People often transform them into numerical values by the following techniques: \n",
    "    - **One-hot encoding:** e.g., you have one column `alphabet` having 3 values `a`, `b`, and `c`. This technique will create 3 more columns `alphabet_a`, `alphabet_b`, and `alphabet_c`. Now, if a row containing the value `a` in the column `alphabet`, the values of the 3 columns `alphabet_a`, `alphabet_b`, `alphabet_c` for this will be `1`, `0`, `0`. Using this technique can lead to an enormous number of columns with a lot of columns containing zero values (`sparse data problem`), be careful!\n",
    "    - **Ordinal encoding:** this is often applied to columns containing values like one, two and three, which imply an `order` in their names. In the case of `one`, `two`, and `three`, they can be translated to `(0, 1, 2)`, or `(1, 2, 3)`.\n",
    "    - **Replace by frequency of observations:** e.g., the value `dog` appears 100 times in the column `animals`. We can replace this value by `100`. \n",
    "    - **Feature Hashing:** Convert into a numerical value by using a hash function. This will return an arbitrary number for each category, which is useful when values do not have ordinal property. This technique is also used for columns containing sensitive information, for example identity number.\n",
    "    - **Weight of Evidence (WoE):** The formula for this is `ln(percentage of good customers who pay back loan/percentage of bad customers who fall behind with paying a loan)`. This technique is mostly used in financial applications such as credit scoring and churn analysis. Please read more about this [here](https://towardsdatascience.com/churn-analysis-information-value-and-weight-of-evidence-6a35db8b9ec5).\n",
    "- **Transform numerical variables:**  \n",
    "    - **Log algorithm**\n",
    "    - **Reciprocal algorithm** \n",
    "    - **Power transform** \n",
    "    - **Box Cox transform** \n",
    "    - **Yeo-Johnson transform** \n",
    "    - **Discretization/Binning**\n",
    "- **Dealing with outliers:** \n",
    "    - **Truncating:** Remove all outliers from the dataset. It can cause a significant decrease in the number of samples. \n",
    "    - **Winsorization:** For example set all the data below 10th percentile to the value at 10th percentile. \n",
    "    - **Zero coding:** Replace values under a threshold with 0, which is often used with non-negative features such as age. \n",
    "    - **Capping:** This can be called top/bottom coding, which means values under or above a threshold with be replace by the `mean` value minus/plus `n` times `standard deviation`\n",
    "- **Derive features from date/datetime columns:** \n",
    "    - Extract `year`, `quarter`, `month`, `day`, `day of week` (from 0 to 6) and `hour` \n",
    "    - `is_weekend`, `is_leap_year`, `is_morning` \n",
    "    - **Cyclic feature encoding:** Using sine and cosine transformation to transform data. It is useful in the case we want to express the differences between (1h and 2h) and (23h and 0h) are same. Please read more [here](https://www.kaggle.com/code/avanwyk/encoding-cyclical-features-for-deep-learning).\n",
    "- **Feature scaling:**\n",
    "    - **Standardization:** `x_scaled = (x - mean)/std`\n",
    "    - **Mean normalization:** `x_scaled = (x - mean)/(max - min)`\n",
    "    - **Min max scaling:** `x_scaled = (x - min)/(max - min)` \n",
    "    - **Max absolute scaling:** `x_scaled = x/max` \n",
    "    - **Vector unit length scaling:** `x_scaled = x/norm1` or `x/norm2`, in which `norm1 = |x1| + |x2| + ...` and `norm2 = sqrt(x1**2 + x2**2 + ...)`\n",
    "- **Invent new features:**\n",
    "    - **Combine multiple features into one** \n",
    "    - **Use decision tree's prediction as one feature:** This is the winning solution of the KDD competition in 2009, the author trained a decision tree on some features and turned its prediction to one feature.\n",
    "- **Dimension reduction:** \n",
    "    - **PCA:** Create new features by finding a new cooridinate system to maximize data variance. \n",
    "    - **LDA:** Similar to PCA, but maximize the seperability between groups instead of maximize data variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c82204-eb6f-49b4-b7de-3ba73e7eb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we have columns containing null values\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2052d41-33c2-4ef3-9ef2-c2c39e7f912d",
   "metadata": {},
   "source": [
    "We can easily see our features are mostly non-null, except some columns such as `MSZoning` having just a few null values, so dropping missing values is not a bad idea. However, a more terrible point we need to pay attention is `SalePrice`, in which about half of them are null. Let's investigate it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a6aba-4c23-4ce2-9e65-d493b997094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"SalePrice\"].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81e060ef-c8aa-46e2-b7d5-a4b236e6de7d",
   "metadata": {},
   "source": [
    "Yes, 1459 rows are NaN in `SalePrice` indeed, we can use the above imputing missing values technique, but with your own risks, since `SalePrice` is the target (dependent variable), not features. Another promising solution is clustering our records, then fill the missing `SalePrice` values with the cluster's center the data point belongs to. To keep it simple, I will try with another simple solution, which is dropping NaN rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e75233-9a79-4381-ab72-db2ebf7f0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df[~df[\"SalePrice\"].isnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c079a01e-c2c8-4da8-a289-fef4d2859244",
   "metadata": {},
   "source": [
    "### 3.2. Train-Val-Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8a16259-7b4a-4379-9dd3-1cf05bd47a1f",
   "metadata": {},
   "source": [
    "In this section, we will split our cleaned data into 3 main parts: training, validation and testing, in which: \n",
    "- **Training:** used for training models \n",
    "- **Validation:** used for validating the trained model and tuning hyperparameters if needed. E.g., you have a linear regression model `y = ax1 + bx2 + c`, you use validation dataset to evaluate the trained model and find it is not good, then you can change the hyperparameter so that your new model can become something like `y = ax1 + b` \n",
    "- **Testing:** used for testing your trained model before deploying your models into a production environment. This dataset shouldn't be used for tuning hyperparameters as the one above.\n",
    "\n",
    "Normally, people will devide the above 3 datasets with the ratio 80%, 10% and 10% respectively. Let's do it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73207f41-f804-4272-857f-9a2a4d0803a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's seperate X (independent features) and y (dependent features) \n",
    "# from the cleaned dataframe first\n",
    "X = cleaned_df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = cleaned_df[\"SalePrice\"]\n",
    "\n",
    "# Do train-val-test-split\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76cc6923-c467-48bc-a963-3f53624413cb",
   "metadata": {},
   "source": [
    "### 3.3. Training Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1490c45-b6ab-4e55-8a44-f17b57f5a233",
   "metadata": {},
   "source": [
    "Our life is now easier, it's time to play around with some models. But first, let's think about what models we are going to use. For a regression problem, we are required to predict continuous values, instead of discrete values in classification. In reality, we often use XGBoost or LightGBM for such regression problems. However, before that, we also need a benchmark, which should be a much simpler model. Take linear regression for this. Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677ea60-cc87-4d6b-8b24-4201ae255722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_features = numeric_df.drop(columns=[\"SalePrice\"]).columns\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = non_numeric_df.columns\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81615c14-68a3-46ee-8f49-4c0c274db8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"*\"*50)\n",
    "print(\"Model score on training set: %.3f\" % clf.score(X_train, y_train))\n",
    "print(\"model score on validation set: %.3f\" % clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816267d-4e09-4d21-95d9-b738ed699451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try with LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'verbose': 1,\n",
    "    \"max_depth\": 4,\n",
    "    \"num_leaves\": 16,\n",
    "}\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"lightgbm\", lgb.LGBMRegressor(**hyper_params))]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Model score on training set: %.3f\" % clf.score(X_train, y_train))\n",
    "print(\"model score on validation set: %.3f\" % clf.score(X_val, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6bf3102-083b-48e5-a82a-9241caa66191",
   "metadata": {},
   "source": [
    "Pretty cool! But tuning hyperparameters for LightGBM manually is terrible, try using an automatic tuning method such as GridSearch or Optuna. ;)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca5bdcdd-f632-47e2-a9b1-b1fe01f23fb7",
   "metadata": {},
   "source": [
    "### 3.4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357056ba-1a61-408e-9f73-ed855902412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, evaluate on our test set before releasing\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "# Get a prediction\n",
    "print(f\"House Attributes: {X_test.iloc[:1].to_dict()}\")\n",
    "print(f\"Predicted: {clf.predict(pd.DataFrame(X_test.iloc[:1]))[0]}\")\n",
    "print(f\"Groundtruth: {y_test.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf013f-5b0a-431c-828f-edcd7baf1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "366566ed-e3bc-4da1-934d-9586a8c238e9",
   "metadata": {},
   "source": [
    "### 3.5. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48eae7-e6c6-4c16-8d08-a05251486bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export model to pickle format using Joblib\n",
    "\"\"\"\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "\n",
    "os.makedirs(MODEL_DIR)\n",
    "joblib.dump(clf, f\"./{MODEL_DIR}/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b18d9c-2cc9-4c1a-b06b-3d9d07d2a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reload model to make prediction\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Export model to pickle format using Joblib\n",
    "\"\"\"\n",
    "MODEL_DIR = \"../models\"\n",
    "clf = joblib.load(f\"./{MODEL_DIR}/model.pkl\")\n",
    "\n",
    "print(f\"Predicted: {clf.predict(pd.DataFrame(X_test.iloc[:1]))[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266ee68d-758d-4f11-8e54-fa76edb73e25",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "129599c1-5765-4a9e-b108-ad07e418d394",
   "metadata": {},
   "source": [
    "Phhh... We have gone through a lot of steps in this notebook, but it's just a small part of a Data Science/ML project. We will need something automatically such as data processing pipelines instead of manually conducting EDA and processing each column. An automatic and scalable tuning and training pipeline will also be needed to reduce an extensive time locally. But for now, let it be. We will revise all these steps and move each of them into production in module 3 of this course :). Cheers!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b820395-4732-49a8-b3b0-574be5202d6c",
   "metadata": {},
   "source": [
    "### 5. Future Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afd22a9-0771-48b2-966b-c2412966d855",
   "metadata": {},
   "source": [
    "I have proposed a lot of ideas in this notebook, it's time for you to beat my score ;)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
